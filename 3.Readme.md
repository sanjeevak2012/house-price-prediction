## Goal
- Python files automate trying multiple parameter combinations for each model, track performance via MLflow, and let you compare results to pick the best model. 

This project demonstrates a simple end-to-end MLflow workflow for training, logging, deploying, and testing a regression model on house‐price data. Think of it like a small factory:  
1. **data.py** gathers and prepares raw materials (data).  
2. **utils.py** acts as the quality‐checker, measuring how well our “product” (model) performs.  
3. **params.py** lists different “recipes” (hyperparameter grids) to try.  
4. **train.py** is the “chef” that mixes ingredients according to each recipe, logs results, and packages the model.  
5. **run.py** is like the “conductor” that tells MLflow to kick off the training process.  
6. **deploy.py** ships the finished product (model) to SageMaker.  
7. **test.py** is the “QA inspector” that sends test samples to the deployed endpoint and prints predictions.

Below is a concise, analogy‐based explanation of each file, in the order they’re used.

---

## data.py
_Prepares raw data (ingredients) for training and testing._

- **Reading datasets**  
  - Loads `train.csv` and `test.csv` (raw ingredients) into pandas DataFrames.  
- **Splitting & imputing**  
  - Splits training data into `X_train`, `X_val`, `y_train`, `y_val` (separates batches).  
  - Uses KNNImputer to “fill in missing bits” of numeric features.  
  - Fills missing categorical values with the most frequent category (mode).  
- **One‐Hot Encoding**  
  - Transforms categorical columns into numeric “flavors” using `OneHotEncoder` (so models can digest them).  
- **Outputs**  
  - Final `X_train`, `X_val`, `y_train`, `y_val`, and processed `test` ready for modeling.

---

## utils.py
_Evaluates model performance (quality‐checker)._

- **eval_metrics(y_true, y_pred)**  
  - Computes RMSE (“average error magnitude”), MAPE (“percentage error”), and R² (“how well predictions fit”).  
  - Returns a dict:  
    - `"RMSE"`: Root Mean Squared Error  
    - `"MAPE"`: Mean Absolute Percentage Error  
    - `"R2"`: R² score  

---

## params.py
_Defines hyperparameter “recipes” to try (cookbook)._

- **ridge_param_grid**  
  - Grid for Ridge regression:  
    - `alpha`: [0.1, 1.0, 10.0]  
    - `fit_intercept`: [True, False]
- **elasticnet_param_grid**  
  - Grid for ElasticNet:  
    - `alpha`: [0.1, 1.0, 10.0]  
    - `l1_ratio`: [0.2, 0.5, 0.8]  
    - `fit_intercept`: [True, False]
- **xgb_param_grid**  
  - Grid for XGBoost Regressor (more complex recipe):  
    - `n_estimators`, `learning_rate`, `max_depth`, etc.  

---

## train.py
_“Chef” that tries each recipe, cooks (trains) a model, logs results, and packages it._

- **Imports & Setup**
  - Loads MLflow for run tracking and logging.
  - Imports data splits (`X_train`, `X_val`, `y_train`, `y_val`, `test`) from `data.py`.
  - Imports hyperparameter grids (`ridge_param_grid`, `elasticnet_param_grid`, `xgb_param_grid`) from `params.py`.
  - Imports `eval_metrics()` from `utils.py` to compute RMSE, MAPE, and R².

- **Model Loop & MLflow Runs**
  - Iterates over each hyperparameter combination for a chosen model (e.g., `ElasticNet`).
  - For each combo:
    1. **Start MLflow run**:  
       ```python
       with mlflow.start_run():
       ```
    2. **Initialize & Train**:  
       - `model = ElasticNet(**params)`  
       - `model.fit(X_train, y_train)`
    3. **Validation & Metrics**:  
       - `y_pred = model.predict(X_val)`  
       - `metrics = eval_metrics(y_val, y_pred)`
    4. **MLflow Logging**:  
       - Log datasets as artifacts (e.g., `train.csv`, `val.csv`).  
       - `mlflow.log_params(params)` (hyperparameters).  
       - `mlflow.log_metrics(metrics)` (RMSE, MAPE, R²).  
       - `mlflow.sklearn.log_model(model, "elasticnet_model", input_example=…, code_paths=[…])`

- **Why This Structure?**
  - **Experimentation & Iteration**:  
    - Automates trying multiple hyperparameter sets without manual nested loops.
  - **Logging Everything**:  
    - Tracks parameters, metrics, artifacts, and model code in MLflow UI for easy comparison.
  - **Separation of Concerns**:  
    - `train.py` focuses strictly on training + logging; runtime details (tracking URI, experiment name) are specified externally (e.g., in `run.py`).

- **Analogy (Factory)**
  - **Conveyor Belt**: Each MLflow run is a station packaging one “dish” (model) with its “recipe card” (params) and “taste scores” (metrics).  
  - **Quality Control**: `eval_metrics()` acts like a QC inspector reviewing each dish before it’s logged.

---

## run.py
_Trigger to launch the entire cooking process under MLflow’s supervision._

- **Configuration**  
  - Sets `experiment_name = "ElasticNet"`.  
  - Specifies `entry_point = "Training"`.  
  - Points MLflow to its tracking server at `http://127.0.0.1:5000`.  
- **Runs the MLflow project**  
  - Calls `mlflow.projects.run(uri=".", entry_point="Training", ...)`.  
  - Uses Conda to recreate the environment based on `conda.yaml`.  

> **Analogy:** Think of this as the “press play” button that tells MLflow: “Run the training workflow defined in this folder.”

---

## deploy.py
_Ships the finished model to AWS SageMaker as a live endpoint (shipping desk)._

- **Variables**  
  - `endpoint_name = "prod-endpoint"`.  
  - `model_uri` points to the model artifact on S3 (the stored, finalized dish).  
- **Configuration dict**  
  - Contains AWS IAM role, S3 bucket, Docker image URL, AWS region, instance specs, etc.  
- **Deployment client**  
  - `client = get_deploy_client("sagemaker")` (interacts with SageMaker).  
- **Create the deployment**  
  - `client.create_deployment(name, model_uri, flavor="python_function", config=config)`.  

> **Analogy:** You send the packaged dish (model) and tell SageMaker “Unpack and serve it on these cooking stations (instances).”

---

## test.py
_QA inspector that sends sample data to the live endpoint and prints predictions._

1. **Imports**  
   - `test` data matrix from `data.py` (first 20 rows).  
   - Boto3 clients for SageMaker and runtime.  
2. **Prepare test payload**  
   - Converts first 20 test rows (after one-hot encoding) to JSON:  
     ```python
     test_data_json = json.dumps({'instances': test[:20].toarray()[:, :-1].tolist()})
     ```  
3. **Invoke endpoint**  
   - Calls `smrt.invoke_endpoint(EndpointName=..., Body=test_data_json, ContentType='application/json')`.  
   - Decodes and prints the prediction results.  

> **Analogy:** Sends 20 taste samples from the test batch to the restaurant (endpoint) and prints out how the chef (model) responds with predicted prices.

---

### Workflow Sequence
1. **data.py** prepares ingredients → produces `(X_train, X_val, y_train, y_val, test)`.  
2. **utils.py** provides a “taste test” function `eval_metrics(...)`.  
3. **params.py** lists all “recipes” (hyperparameter grids).  
4. **train.py** loops through recipes, cooks models, logs everything in MLflow.  
5. **run.py** is the single command to trigger `train.py` under MLflow.  
6. **deploy.py** takes the best logged model from S3 and deploys it on SageMaker as `prod-endpoint`.  
7. **test.py** sends sample data to `prod-endpoint` and prints out predictions.

---

#### Analogy Summary
- **Ingredients & Prep (data.py)** → gather and clean raw data.  
- **Recipes (params.py)** → define possible ways to cook.  
- **Chef (train.py + utils.py)** → train models, record results.  
- **Press “Go” (run.py)** → kick off the training factory.  
- **Shipping (deploy.py)** → deploy the best model as a service.  
- **QA (test.py)** → send test samples to the deployed service and inspect outputs.

---

## Use of `conda.yaml` and `MLproject` Files

---

## `conda.yaml`
- **Purpose**: Define a generic, system‐independent list of dependencies.
- **Why Generic?**  
  - Exported Conda files may include native or platform‐specific packages.  
  - A generic file ensures compatibility across local machines and AWS SageMaker.
- **Contents**:  
  - Only the essential libraries required for this project (e.g., `pandas`, `scikit-learn`, `xgboost`).  
  - Versions that are likely available on any system.
- **How to Prepare**:  
  1. Ensure all required libraries are installed locally (e.g., `pip install xgboost`).  
  2. Manually list dependencies and their versions in `conda.yaml` (avoid platform‐specific packages).  
  3. Place `conda.yaml` in the project directory.

---

## `MLproject`
- **Purpose**: Instruct MLflow how to launch and manage the training workflow.
- **Structure**:  
  1. **`name`**: Project identifier (e.g., `name: MLproject`).  
  2. **`conda_env`**: Path to `conda.yaml`, specifying the environment.  
  3. **`entry_points`**: Defines a single entry point for training.
     - **`main`** (or custom name):  
       - Command to run: `python train.py`.
- **Why It Matters**:  
  - Standardizes how MLflow starts the project, ensuring reproducibility.  
  - Automatically recreates the correct environment from `conda.yaml`.
- **How to Run**:  
  1. **Locally**:  
     - Use `mlflow run . --entry-point main` or a Python script (e.g., `run.py`).  
  2. **AWS SageMaker**:  
     - Later, change `run.py`’s `tracking_uri` to point to SageMaker, then invoke the same entry point.



# Why We Use Ridge, ElasticNet, and XGBRegressor ?

- **Ridge (L2 Regularization)**
  - **Purpose**: Prevents overfitting by penalizing large coefficients.
  - **Analogy**: Like adding a weight to each ingredient so none dominates the recipe.

- **ElasticNet (L1 + L2 Regularization)**
  - **Purpose**: Combines L1 (sparsity) and L2 (shrinkage) to balance feature selection and stability.
  - **Analogy**: Using both salt and pepper—one gives flavor (selects important features), the other keeps everything balanced.

- **XGBRegressor (Gradient Boosting)**
  - **Purpose**: Builds many small decision trees sequentially to capture complex, non-linear patterns.
  - **Analogy**: Having multiple chefs each add a small improvement to the dish, one after another, for better overall taste.

```bash
mlflow sagemaker build-and-push-container --container xgb --env-manager conda
# To build a container for a specific model from a specific MLflow run, you would typically include the --model-uri flag, like this:
mlflow sagemaker build-and-push-container --model-uri runs:/<run_id>/model --container xgb --env-manager conda
#  if you've registered your model in the MLflow Model Registry:
mlflow sagemaker build-and-push-container --model-uri models:/<model_name>/<model_version_or_stage> --container xgb --env-manager conda
```
**Note:** All explanations above are intentionally kept short, with analogies to simplify each step. Follow the sequence to understand how raw data becomes a deployed, production‐ready ML model.

